{
    "collab_server" : "",
    "contents" : "## Code for examples from Day 2 of text analysis\n## Ken Benoit <kbenoit@lse.ac.uk>\n\nrequire(quanteda)\n\n##\n## hiearchical clustering\n##\ndata(SOTUCorpus, package = \"quantedaData\")\n# create a DFM after removing stopwords and stemming\npresDfm <- dfm(subset(SOTUCorpus, Date > as.Date(\"1960-01-01\")), \n               ignoredFeatures = stopwords(\"english\"), stem = TRUE)\n# reduce the feature set through thresholds\npresDfm <- trim(presDfm, minCount = 5, minDoc = 3)\n\n## hierarchical clustering\n# get distances on normalized dfm\npresDistMat <- dist(as.matrix(weight(presDfm, \"relFreq\")))\n# hiarchical clustering the distance object\npresCluster <- hclust(presDistMat)\n# label with document names\npresCluster$labels <- docnames(presDfm)\n# plot as a dendrogram\nplot(presCluster)\n\n## hierarchical clustering on words\n# weight by relative term frequency\nwordDfm <- sort(tf(presDfm, \"prop\"))  # sort in decreasing order of total word freq\nwordDfm <- t(wordDfm)[1:100, ]  \nwordDistMat <- dist(wordDfm)\nwordCluster <- hclust(wordDistMat)\nplot(wordCluster, labels = docnames(wordDfm),\n     xlab=\"\", main=\"Relative Term Frequency weighting\")\n\n# repeat without word \"will\"\nwordDfm <- removeFeatures(wordDfm, \"will\")\nwordDistMat <- dist(wordDfm)\nwordCluster <- hclust(wordDistMat)\nplot(wordCluster, labels = docnames(wordDfm), \n     xlab=\"\", main=\"Relative Term Frequency without \\\"will\\\"\")\n\n# with tf-idf weighting\nwordDfm <- sort(tf(presDfm, \"prop\"))  # sort in decreasing order of total word freq\nwordDfm <- removeFeatures(wordDfm, c(\"will\", \"year\", \"s\"))\nwordDfm <- t(wordDfm)[1:100,]  # because transposed\nwordDistMat <- dist(wordDfm)\nwordCluster <- hclust(wordDistMat)\nplot(wordCluster, labels = docnames(wordDfm),\n     xlab=\"\", main=\"tf-idf Frequency weighting\")\n\n\n\n\n##\n## topic models with visualization\n##\n\n# load movies and create a dfm\ndata(movies, package = \"quantedaData\")\n\n# create a document-feature matrix, without stopwords, and with reduced features\nmoviesDfm <- dfm(movies, ignoredFeatures = stopwords(\"SMART\"), stem = FALSE)\nmoviesDfm <- trim(moviesDfm, minCount = 5)\n\n# MCMC and model tuning parameters:\nK <- 20\nG <- 5000\nalpha <- 0.02\neta <- 0.02\n\n# convert to lda format\nmoviesDfmlda <- convert(moviesDfm, to = \"lda\")\n\n# fit the model\nrequire(lda)\nset.seed(357)\nt1 <- Sys.time()\nfit <- lda.collapsed.gibbs.sampler(documents = moviesDfmlda$documents, K = K, \n                                   vocab = moviesDfmlda$vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 11 minutes on Ken's iMac\n# save(fit, file = \"Notes/Day 9 - Topic Models/fit.RData\")\n# load(\"Notes/Day 9 - Topic Models/fit.RData\")\n\nrequire(LDAvis)\n# create the JSON object to feed the visualization:\njson <- createJSON(phi = (fit$topics + eta) / rowSums(fit$topics + eta), \n                   theta = (fit$document_sums + alpha) / rowSums(fit$document_sums + alpha), \n                   doc.length = ntoken(moviesDfm), \n                   vocab = features(moviesDfm), \n                   term.frequency = colSums(moviesDfm))\nserVis(json)\n\n\n\n##\n## redo the topic model substituting collocations for words\n##\n\nmoviesCollocations <- collocations(movies, method = \"lr\", size = 2)\nhead(moviesCollocations, 20)\nmoviesCollocations <- removeFeatures(moviesCollocations, stopwords(\"SMART\"))\nhead(moviesCollocations, 20)\n\n# replace collocations with top 1000 concatenated phrases\n# note: the indexing for moviesCollocations uses the data.table \"[\" method\nmoviesCorpusPhrases <- corpus(phrasetotoken(texts(movies), moviesCollocations[1:1000]))\nmoviesDfmPhrases <- dfm(moviesCorpusPhrases, ignoredFeatures = stopwords(\"SMART\"), stem = TRUE)\nset.seed(0315)\n# convert to lda format\nmoviesDfmPhraseslda <- convert(moviesDfmPhrases, to = \"lda\")\nt1 <- Sys.time()\nfit2 <- lda.collapsed.gibbs.sampler(documents = moviesDfmPhraseslda$documents, K = K, \n                                   vocab = moviesDfmPhraseslda$vocab, \n                                   num.iterations = G, alpha = alpha, \n                                   eta = eta, initial = NULL, burnin = 0,\n                                   compute.log.likelihood = TRUE)\nt2 <- Sys.time()\nt2 - t1  # about 11 minutes on Ken's iMac\n# save(fit2, file = \"Notes/Day 9 - Topic Models/fit2.RData\")\n# load(\"Notes/Day 9 - Topic Models/fit2.RData\")\n\nrequire(LDAvis)\n# create the JSON object to feed the visualization:\njson2 <- createJSON(phi = (fit2$topics + eta) / rowSums(fit2$topics + eta), \n                   theta = (fit2$document_sums + alpha) / rowSums(fit2$document_sums + alpha), \n                   doc.length = ntoken(moviesDfmPhrases), \n                   vocab = features(moviesDfmPhrases), \n                   term.frequency = colSums(moviesDfmPhrases))\nserVis(json2)\n\n\n\n",
    "created" : 1464338414427.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "598392347",
    "id" : "CD6C72D3",
    "lastKnownWriteTime" : 1458245055,
    "last_content_update" : 1458245055,
    "path" : "~/Dropbox (Personal)/00 Imperial College/1601 Machine Learning/Part 3 - Text Mining/Lecture 2/day2examples.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}